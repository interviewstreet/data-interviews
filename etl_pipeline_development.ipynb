{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HackerRank Lead Data Engineer Interview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL Pipeline Development\n",
    "\n",
    "During this interview, we aim to evaluate several skills:\n",
    "1. Transform data using SQL\n",
    "2. Manage dataframes with PySpark or similar technologies\n",
    "3. Identify and troubleshoot inconsistencies in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "When you are solving a test on the HackerRank platform, our platform collects click stream data on certain user actions. For example, we receive ping data when you run code, submit code, or view different questions. In this notebook, you will be manipulating a similar set of the click stream data to extract certain features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .master(\"local\") \\\n",
    "                    .appName(\"etl_pipeline_development\") \\\n",
    "                    .enableHiveSupport() \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### Relevant Tables\n",
    "\n",
    "\\* **Perform any cleaning, exploratory analysis, and/or visualizations for the provided data as needed.**\n",
    "\n",
    "There are several `csv` files we will be using for this exercise:\n",
    "1. `ping_events.csv`\n",
    "2. `company_candidates.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "\n",
    "`ping_events` schema:\n",
    "\n",
    "|  column name  |  data type  |\n",
    "|---------------|-------------|\n",
    "|  attempt_id   |  int        |\n",
    "|  event_id     |  int        |\n",
    "|  inserttime   |  datetime   |\n",
    "|  metadata     |  string     |\n",
    "\n",
    "In the `metadata` column, `qno` represents the question number within the test and `question_id` represents the question id stored in our database.\n",
    "\n",
    "Sample rows from `ping_events`:\n",
    "\n",
    "|  attempt_id  |  event_id  |  inserttime  |  metadata                      |\n",
    "|:------------:|:----------:|:------------:|:-------------------------------|\n",
    "|      1       |  2         |  05:40       | {\"question_id\": 101, \"qno\": 1} |\n",
    "|      1       |  2         |  05:45       | {\"question_id\": 103, \"qno\": 3} |\n",
    "|      1       |  3         |  05:46       | {\"question_id\": 204, \"qno\": 2} |\n",
    "|      1       |  3         |  05:50       | {\"question_id\": 101, \"qno\": 1} |\n",
    "|      1       |  1         |  05:55       | {\"qno\": 0}                     |\n",
    "|      2       |  3         |  06:20       | {\"question_id\": 103, \"qno\": 2} |\n",
    "|      2       |  3         |  06:50       | {\"question_id\": 101, \"qno\": 1} |\n",
    "|      2       |  1         |  07:10       | {\"qno\": 0}                     |\n",
    "\n",
    "Write executable PySpark SQL to create a table with `event_id`, `inserttime`, and `metadata` as an array of tuples grouped by `attempt_id` in a column named `data`:\n",
    "\n",
    "\\* **Note that the data is in array form under one `attempt_id`**\n",
    "\n",
    "|   attempt_id   |   data                                       |\n",
    "|:--------------:|:---------------------------------------------|\n",
    "|       15       | [                                            |\n",
    "|                |  (2, 05:40, {\"question_id\": 101, \"qno\": 1}), |\n",
    "|                |  (2, 05:45, {\"question_id\": 103, \"qno\": 3}), |\n",
    "|                |  (3, 05:46, {\"question_id\": 204, \"qno\": 2}), |\n",
    "|                |  (3, 05:50, {\"question_id\": 101, \"qno\": 1}), |\n",
    "|                |  (1, 05:55, {\"qno\": 0})                      |\n",
    "|                | ]                                            |\n",
    "|       16       | [                                            |\n",
    "|                |  (3, 06:20, {\"question_id\": 103, \"qno\": 2}), |\n",
    "|                |  (3, 06:50, {\"question_id\": 101, \"qno\": 1}), |\n",
    "|                |  (1, 07:10, {\"qno\": 0})                      |\n",
    "|                | ]                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "\n",
    "Using PySpark, process the resulting table above to determine the time spent on each question. The dictionaries should follow this format: `{question_id: time_in_seconds}`. Do not use Pandas to perform this transformation.\n",
    "\n",
    "Each `inserttime` event denotes a user-action. The method we use to calculate the `time_spent` transformation is by looking at the difference between consecutive ping `inserttime`s. For attempt 15, the candidate first looked at `qno 1` at 05:40 then `qno 3` at 05:45. So we know the user spent 5 minutes so far on `qno 1`.\n",
    "\n",
    "Using this method, the above table should be transformed into:\n",
    "\n",
    "|   attempt_id   |   time_spent              |\n",
    "|:--------------:|:--------------------------|\n",
    "|     15         | {\"1\": 10, \"2\": 4, \"3\": 1} |\n",
    "|     16         | {\"1\": 20, \"2\": 30}        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "\n",
    "We want to create a table with `company_id` as well:\n",
    "\n",
    "|   company_id   |   attempt_id   |   time_spent              |\n",
    "|:--------------:|:--------------:|:--------------------------|\n",
    "|   1            |     15         | {\"1\": 10, \"2\": 4, \"3\": 1} |\n",
    "|   2            |     16         | {\"1\": 20, \"2\": 30}        |\n",
    "\n",
    "To get the `company_id` associated with each `attempt_id`, you can use `company_candidates.csv`. After you generate the above table, please store it as an interim table called `attempt_times`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
